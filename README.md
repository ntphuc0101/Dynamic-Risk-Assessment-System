# Dynamic-Risk-Assessment-System
Imagine that you're the Chief Data Scientist at a big company that has 10,000 corporate clients. Your company is extremely concerned about attrition risk: the risk that some of their clients will exit their contracts and decrease the company's revenue. They have a team of client managers who stay in contact with clients and try to convince them not to exit their contracts. However, the client management team is small, and they're not able to stay in close contact with all 10,000 clients.

## Prerequisites
- Python 3 required

## Dependencies
This project dependencies is available in the ```requirements.txt``` file.

## Installation
Use the package manager [pip](https://pip.pypa.io/en/stable/) to install the dependencies from the ```requirements.txt```. Its recommended to install it in a separate virtual environment.

```bash
pip install -r requirements.txt
```

## Project Structure
```bash
ðŸ“¦Dynamic-Risk-Assessment-System
 â”£
 â”£ ðŸ“‚data
 â”ƒ â”£ ðŸ“‚ingesteddata                 # Contains csv and metadata of the ingested data
 â”ƒ â”ƒ â”£ ðŸ“œfinaldata.csv
 â”ƒ â”ƒ â”£ ðŸ“œfinaldata.csv
 â”ƒ â”ƒ â”— ðŸ“œlatestscore.txt
 â”ƒ â”£ ðŸ“‚practicedata                 # Data used for practice mode initially
 â”ƒ â”ƒ â”£ ðŸ“œdataset1.csv
 â”ƒ â”ƒ â”— ðŸ“œdataset2.csv
 â”ƒ â”£ ðŸ“‚sourcedata                   # Data used for production mode
 â”ƒ â”ƒ â”£ ðŸ“œdataset3.csv
 â”ƒ â”ƒ â”— ðŸ“œdataset4.csv
 â”ƒ â”— ðŸ“‚testdata                     # Test data
 â”ƒ â”ƒ â”— ðŸ“œtestdata.csv
 â”£ ðŸ“‚model
 â”ƒ â”£ ðŸ“‚models                       # Models pickle, score, and reports for production mode
 â”ƒ â”ƒ â”£ ðŸ“œapireturns.txt
 â”ƒ â”ƒ â”£ ðŸ“œconfusionmatrix.png
 â”ƒ â”ƒ â”£ ðŸ“œlatestscore.txt
 â”ƒ â”ƒ â”£ ðŸ“œsummary_report.pdf
 â”ƒ â”ƒ â”— ðŸ“œtrainedmodel.pkl
 â”ƒ â”£ ðŸ“‚practicemodels               # Models pickle, score, and reports for practice mode
 â”ƒ â”ƒ â”£ ðŸ“œapireturns1.txt
 â”ƒ â”ƒ â”£ ðŸ“œconfusionmatrix1.png
 â”ƒ â”ƒ â”£ ðŸ“œlatestscore.txt
 â”ƒ â”ƒ â”— ðŸ“œtrainedmodel.pkl
 â”ƒ â”— ðŸ“‚production_deployment        # Deployed models and model metadata needed
 â”ƒ â”ƒ â”£ ðŸ“œingestedfiles.txt
 â”ƒ â”ƒ â”£ ðŸ“œlatestscore.txt
 â”ƒ â”ƒ â”£ ðŸ“œencoder.pkl
 â”ƒ â”ƒ â”— ðŸ“œtrainedmodel.pkl
 â”ƒ â”ƒ
 â”ƒ ðŸ“œapicalls.py                  # Runs app endpoints
 â”ƒ ðŸ“œapp.py                       # Flask app
 â”ƒ ðŸ“œconfig.py                    # Config file for the project which depends on config.json
 â”ƒ ðŸ“œdeployment.py                # Model deployment script
 â”ƒ ðŸ“œdiagnostics.py               # Model diagnostics script
 â”ƒ ðŸ“œfullprocess.py               # Process automation
 â”ƒ ðŸ“œingestion.py                 # Data ingestion script
 â”ƒ ðŸ“œpretty_confusion_matrix.py   # Plots confusion matrix
 â”ƒ ðŸ“œreporting.py                 # Generates confusion matrix and PDF report
 â”ƒ ðŸ“œscoring.py                   # Scores trained model
 â”ƒ ðŸ“œtraining.py                  # Model training
 â”ƒ ðŸ“œwsgi.py
 â”£ ðŸ“œconfig.json                    # Config json file
 â”£ ðŸ“œcronjob.txt                    # Holds cronjob created for automation
 â”£ ðŸ“œREADME.md
 â”£ ðŸ“œcron
 â”— ðŸ“œrequirements.txt               # Projects required dependencies
```

## Project Steps Overview
1. **Data ingestion:** Automatically check if new data that can be used for model training. Compile all training data to a training dataset and save it to folder. 
2. **Training, scoring, and deploying:** Write scripts that train an ML model that predicts attrition risk, and score the model. Saves the model and the scoring metrics.
3. **Diagnostics:** Determine and save summary statistics related to a dataset. Time the performance of some functions. Check for dependency changes and package updates.
4. **Reporting:** Automatically generate plots and PDF document that report on model metrics and diagnostics. Provide an API endpoint that can return model predictions and metrics.
5. **Process Automation:** Create a script and cron job that automatically run all previous steps at regular intervals.

<img src="pipline_dynamic.png" width=550 height=300>

## Usage

### 1- Edit config.json file to use practice data

```bash
"input_folder_path": "practicedata",
"output_folder_path": "ingesteddata", 
"test_data_path": "testdata", 
"output_model_path": "practicemodels", 
"prod_deployment_path": "production_deployment"
```

### 2- Run data ingestion
```python
cd src
python ingestion.py
```

### 3- Model training
```python
python training.py
```

###  4- Model scoring 
```python
python scoring.py
```

### 5- Model deployment
```python
python deployment.py
```
### 6- Run diagnostics
```python
python diagnostics.py
```

### 7- Run reporting
```python
python reporting.py
```
### 8- Run Flask App
```python
python app.py
```

### 9- Run API endpoints
```python
python apicalls.py
```

### 11- Edit config.json file to use production data

```bash
"input_folder_path": "sourcedata",
"output_folder_path": "ingesteddata", 
"test_data_path": "testdata", 
"output_model_path": "models", 
"prod_deployment_path": "production_deployment"
```

### 10- Full process automation
```python
python fullprocess.py
```
### 11- Cron job

Start cron service
```bash
sudo service cron start
```

Edit crontab file
```bash
sudo crontab -e
```
   - Select **option 3** to edit file using vim text editor
   - Press **i** to insert a cron job
   - Write the cron job in ```cronjob.txt``` which runs ```fullprocces.py``` every 10 mins
   - Save after editing, press **esc key**, then type **:wq** and press enter